## from the supplied data will be used
mydata.lda <- lda(Sp ~ ., Data, prior = c(1,1,1)/3, subset = learn)
## or alternatively (they are the same call)
mydata.lda <- lda(Sp ~ ., prior = c(1,1,1)/3, data = mydata.learn)
### LDA can be used for dimension reduction by plotting the data as given by the discriminant functions; these are the g_k(x) as seen in class
## lda prints discriminant functions based on centered (but not standardized) variables
## The "proportion of trace" that is printed is the proportion of between-class variance that is explained by successive discriminant functions
## the list element 'class' gives the predicted class
## The list element 'posterior' holds posterior probabilities
mydata.lda
# Now we project the data in two new axes (scatter plot using the first two discriminant dimensions)
# These new dimensions are linear combinations of the original 4 measurements
plot(mydata.lda)
# We can explicitly compute the projected data (the new coordinates)
# To illustrate, we just compute projections onto the first LD
# for LDA, this amounts to y = w_1^T x, where w_1 is LD1 in R
d1LDA <- mydata.learn[,1] * mydata.lda$scaling[1] + mydata.learn[,2] * mydata.lda$scaling[2] +
mydata.learn[,3] * mydata.lda$scaling[3] + mydata.learn[,4] * mydata.lda$scaling[4]
# which corresponds to the LD1 axis on the previous plot
### PART2: Using LDA for classification
## comparing LDA's prediction with the targets in the data, we get the apparent (a.k.a. training) error (optimistic!)
predict(mydata.lda, mydata.learn)$class
## we may also have a look at
predict(mydata.lda, mydata.learn)$posterior # the posterior probabilities
## Assessment of predictive accuracy via LOOCV (leave-one-out cross-validation)
# Some form of cross-validation is necessary if we want to select one model against others
# The idea is to extract one example, build a model with the remaining N-1 and make the model predict the left out example.
# Then change the example and repeat analogously for all of them. As you can see, this process creates N validation sets of size 1.
# The reported result is the fraction of errors (wrongly predicted examples)
# If CV=TRUE, the method returns results (classes and posterior probabilities) for LOOCV.
# Note that if the prior is estimated, the proportions in the whole dataset are used.
mydata.lda.cv <- lda(Sp ~ ., prior = c(1,1,1)/3, data = Data, subset=learn, CV=TRUE)
summary(mydata.lda.cv$class)
tab <- table(Data$Sp[learn], mydata.lda.cv$class)
(error.LOOCV <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
tab
(tab <- table(Data$Sp[learn], mydata.lda.cv$class))
(error.LOOCV <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
mydata.lda.cv.noPetal.W <- update(mydata.lda.cv, . ~ . - Petal.W.)
(tab <- table(Data$Sp[learn], mydata.lda.cv.noPetal.W$class))
(error.LOOCV <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
## Now switch to QDA
mydata.qda.cv <- qda(Sp ~ ., prior = c(1,1,1)/3, data = Data, subset=learn, CV=TRUE)
summary(mydata.qda.cv$class)
(tab <- table(Data$Sp[learn], mydata.lda.cv$class))
(error.LOOCV <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
mydata.lda <- lda(Sp ~ ., Data, prior = c(1,1,1)/3, subset = learn)
## and use it to predict the test set (we get an estimation of true error)
## Now create the predictions in the test set
lda.predictions <- predict(mydata.lda, mydata.test)
lda.predictions$class
## and compute the error
(tab <- table(Data$Sp[-learn], lda.predictions$class))
(error.TE <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
wine <- read.table("wine.data", sep=",", dec=".", header=FALSE)
wine <- read.table("Wine/wine.data", sep=",", dec=".", header=FALSE)
dim(wine)
colnames(wine) <- c('Wine.type','Alcohol','Malic.acid','Ash','Alcalinity.of.ash','Magnesium','Total.phenols',
'Flavanoids','Nonflavanoid.phenols','Proanthocyanins','Color.intensity','Hue','OD280/OD315','Proline')
wine$Wine.type <- as.factor(wine$Wine.type)
summary(wine)
plot(subset(wine,select=-Wine.type),col=unclass(wine$Wine.type))
lda.model <- lda (Wine.type ~ ., data = wine)
lda.model
## Plot the projected data in the first two LDs
## We can see that the discrimination is very good
plot(lda.model)
# alternatively, we can do it ourselves, with more control on color and text (wine number)
wine.pred <- predict(lda.model)
plot(wine.pred$x,type="n")
text(wine.pred$x,labels=as.character(rownames(wine.pred$x)),col=as.integer(wine$Wine.type))
legend('bottomright', c("Cultivar 1","Cultivar 2","Cultivar 3"), lty=1, col=c('black', 'red', 'green'), bty='n', cex=.75)
# If need be, we can add the (projected) means to the plot
plot.mean <- function (class)
{
m1 <- mean(subset(wine.pred$x[,1],wine$Wine.type==class))
m2 <- mean(subset(wine.pred$x[,2],wine$Wine.type==class))
print(c(m1,m2))
points(m1,m2,pch=16,cex=2,col=as.integer(class))
}
plot.mean ('1')
plot.mean ('2')
plot.mean ('3')
# indeed classification is perfect
table(wine$Wine.type, wine.pred$class)
# Let us switch to leave-one-out cross-validation
wine.predcv <- update(lda.model,CV=TRUE)
head(wine.predcv$posterior)
print(table(wine$Wine.type,wine.predcv$class))
qda.model <- qda (Wine.type ~ ., data = wine)
qda.model
wine.pred <- predict(qda.model)
table(wine$Wine.type, wine.pred$class)
# Let us switch to leave-one-out cross-validation
wine.predcv <- update(qda.model,CV=TRUE)
head(wine.predcv$posterior)
print(table(wine$Wine.type,wine.predcv$class))
# 1 mistake (on 178 observations): 0.56% error
# it would be nice to ascertain which wine is the "stubborn" one: it is a wine of type '2' classified
# as class '1'. Maybe there is something strange with this wine ...
####################################################################
Admis <- read.csv("Admissions.csv")
## view the first few rows of the data
head(Admis)
summary(Admis)
Admis$admit <- factor(Admis$admit, labels=c("No","Yes"))
Admis$rank <- ordered(Admis$rank)
summary(Admis)
xtabs(~admit + rank, data = Admis)
N <- nrow(Admis)
## We first split the available data into learning and test sets, selecting randomly 2/3 and 1/3 of the data
## We do this for a honest estimation of prediction performance
learn <- sample(1:N, round(2*N/3))
nlearn <- length(learn)
ntest <- N - nlearn
# First we build a maximal model using the learn data
Admis.logreg <- glm (admit ~ gre + gpa + rank, data = Admis[learn,], family = "binomial")
summary(Admis.logreg)
## gre is not statistically significant (although gpa is but by a small margin); the three terms for rank
## are all statistically significant. Guess why rank1 is not considered in the model?
## Then we try to simplify the model by eliminating the least important variables progressively
## using the step() algorithm which penalizes models based on the AIC value
## This value is the sum of the error plus twice the number of parameters
Admis.logreg.step <- step (Admis.logreg)
# in this case no variable is removed!, but here is the general code to refit the model:
Admis.logreg <- glm (Admis.logreg.step$formula, data = Admis[learn,], family=binomial)
summary(Admis.logreg)
### INFERENCE PART
## We can interpret the coefficients
exp(Admis.logreg$coefficients)
plot (Admis.logreg$linear.predictors, Admis.logreg$fitted.values, col=Admis[,"admit"])
glfpred<-NULL
glfpred[Admis.logreg$fitted.values<0.5]<-0
glfpred[Admis.logreg$fitted.values>=0.5]<-1
tab <- with(Admis, table(admit[learn],glfpred))
tab
error.learn <- 100*(1-sum(diag(tab))/nlearn)
error.learn
glft <- predict(Admis.logreg, newdata=Admis[-learn,], type="response")
glfpredt <- NULL
glfpredt[glft<0.5]<-0
glfpredt[glft>=0.5]<-1
tab <- with(Admis, table(admit[-learn],glfpredt))
tab
error.test <- 100*(1-sum(diag(tab))/ntest)
error.test
set.seed(1111)
Admis <- read.csv("Admissions.csv")
## view the first few rows of the data
head(Admis)
summary(Admis)
## We will treat the variables gre and gpa as continuous. The variable rank takes on the values 1 through 4
## we treat it as categorical
Admis$admit <- factor(Admis$admit, labels=c("No","Yes"))
Admis$rank <- factor(Admis$rank)
summary(Admis)
## two-way contingency table of outcome and rank,
## we want to make sure the data is OK (no zeros or something strange)
xtabs(~admit + rank, data = Admis)
N <- nrow(Admis)
## We first split the available data into learning and test sets, selecting randomly 2/3 and 1/3 of the data
## We do this for a honest estimation of prediction performance
learn <- sample(1:N, round(2*N/3))
nlearn <- length(learn)
ntest <- N - nlearn
# First we build a maximal model using the learn data
Admis.logreg <- glm (admit ~ gre + gpa + rank, data = Admis[learn,], family = "binomial")
summary(Admis.logreg)
## gre is not statistically significant (although gpa is but by a small margin); the three terms for rank
Admis.logreg.step <- step (Admis.logreg)
# in this case no variable is removed!, but here is the general code to refit the model:
Admis.logreg <- glm (Admis.logreg.step$formula, data = Admis[learn,], family=binomial)
summary(Admis.logreg)
### INFERENCE PART
## We can interpret the coefficients
exp(Admis.logreg$coefficients)
0.0022*100
plot (Admis.logreg$linear.predictors, Admis.logreg$fitted.values, col=Admis[,"admit"])
## let us first calculate the prediction error in the learn data
glfpred<-NULL
glfpred[Admis.logreg$fitted.values<0.5]<-0
glfpred[Admis.logreg$fitted.values>=0.5]<-1
tab <- with(Admis, table(admit[learn],glfpred))
tab
error.learn <- 100*(1-sum(diag(tab))/nlearn)
error.learn
glft <- predict(Admis.logreg, newdata=Admis[-learn,], type="response")
glfpredt <- NULL
glfpredt[glft<0.5]<-0
glfpredt[glft>=0.5]<-1
(tab <- with(Admis, table(admit[-learn],glfpredt)))
error.test <- 100*(1-sum(diag(tab))/ntest)
error.test
table(Admis$admit)[2]/N
library(kernlab)
data(spam)
## We do some basic preprocessing
spam[,55:57] <- as.matrix(log10(spam[,55:57]+1))
spam2 <- spam[spam$george==0,]
spam2 <- spam2[spam2$num650==0,]
spam2 <- spam2[spam2$hp==0,]
spam2 <- spam2[spam2$hpl==0,]
george.vars <- 25:28
spam2 <- spam2[,-george.vars]
moneys.vars <- c(16,17,20,24)
spam3 <- data.frame( spam2[,-moneys.vars], spam2[,16]+spam2[,17]+spam2[,20]+spam2[,24])
colnames(spam3)[51] <- "about.money"
dim(spam3)
summary(spam3)
set.seed (4321)
N <- nrow(spam3)
learn <- sample(1:N, round(0.67*N))
nlearn <- length(learn)
ntest <- N - nlearn
## Fit a logistic regression in the learning data
spamM1 <- glm (type ~ ., data=spam3[learn,], family=binomial)
spamM1.AIC <- step (spamM1)
# 'P' is a parameter; whenever our filter assigns spam with probability at least P then we predict spam
spam.accs <- function (P=0.5)
{
## Compute accuracy in learning data
spamM1.AICpred <- NULL
spamM1.AICpred[spamM1.AIC$fitted.values<P] <- 0
spamM1.AICpred[spamM1.AIC$fitted.values>=P] <- 1
spamM1.AICpred <- factor(spamM1.AICpred, labels=c("nonspam","spam"))
cat("TRAINING ERROR:")
print(M1.TRtable <- table(Truth=spam3[learn,]$type,Pred=spamM1.AICpred))
print(100*(1-sum(diag(M1.TRtable))/nlearn))
## Compute accuracy in test data
gl1t <- predict(spamM1.AIC, newdata=spam3[-learn,],type="response")
gl1predt <- NULL
gl1predt[gl1t<P] <- 0
gl1predt[gl1t>=P] <- 1
gl1predt <- factor(gl1predt, labels=c("nonspam","spam"))
cat("TESTING ERROR:")
print(M1.TEtable <- table(Truth=spam3[-learn,]$type,Pred=gl1predt))
print(100*(1-sum(diag(M1.TEtable))/ntest))
}
spam.accs()
spam.accs(0.7)
64/792
142/821
81/792
52/821
Spam.LDA.predcv <- lda(type ~ ., data=spam3[learn,],CV=TRUE)
(tab <- table(spam3[learn,]$type, Spam.LDA.predcv$class))
(error.LOOCV <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
Spam.QDA.predcv <- qda(type ~ ., data=spam3[learn,],CV=TRUE)
library(klaR)
Spam.RDA.predcv <- rda (type ~ ., data=spam3[learn,], crossval = TRUE)
Spam.RDA.predcv$error.rate
Spam.RDA.predcv$regularization
RDA.preds <- predict (Spam.RDA.predcv, spam3[-learn,])
addmargins(tab <- table(RDA.preds$class, spam3[-learn,]$type,  dnn=c("Pred", "True")))
sum(tab[row(tab)==col(tab)])/sum(tab)
47/347
set.seed (6046)
## Direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit
## Getting the dataset
deposit <- read.table ("Bank Marketing/bank-full.csv", header=TRUE, stringsAsFactors=TRUE, sep=";")
dim(deposit)
summary(deposit)
ncol(deposit)
colnames(deposit)
table(deposit$job)
# education has 4 values, let's check their frequency
# seems OK
table(deposit$education)
# month looks very suspicious ... but is OK
table(deposit$month)
# highly skewed ...
hist(deposit$duration)
hist(log(deposit$duration))
deposit$duration <- log(deposit$duration+0.001)
# what to do with 'pdays' and 'previous'? it is not clear ... we leave as it is
## The rest seem OK (it would take a careful analysis, and a lot of domain knowledge)
# I rename the target ...
colnames(deposit)[ncol(deposit)] <- "subscribed"
dim(deposit)
colnames(deposit)[ncol(deposit)] <- "subscribed"
dim(deposit)
summary(deposit)
## Let's have a visual inspection of the continuous variables
library(reshape2)
library(ggplot2)
d.cont <- melt(deposit[,c("age","balance","duration","campaign","pdays","previous")])
ggplot(d.cont,aes(x = value)) +
facet_wrap(~variable,scales = "free") + geom_histogram()
## Let's have a visual inspection of the factor variables
d.categ <- melt(deposit, measure.vars=c("job","marital","education","housing","loan","contact","default", "poutcome"))
ggplot(d.categ,aes(x = value)) + facet_wrap(~variable,scales = "free") + geom_bar()
N <- nrow(deposit)
all.indexes <- 1:N
learn.indexes <- sample(1:N, round(2*N/3))
test.indexes <- all.indexes[-learn.indexes]
learn.data <- deposit[learn.indexes,]
nlearn <- length(learn.indexes)
ntest <- N - nlearn
library(MASS)
data(iris3)
Data <- data.frame(rbind(iris3[,,1], iris3[,,2], iris3[,,3]),
Sp = rep(c("s","c","v"), rep(50,3)))
Data
learn <- sample(1:150, 75)
learn
table(Data$Sp[learn])
mydata.learn <- Data[learn, ]
mydata.test <- Data[-learn, ]
pairs(Data[c("Sepal.L.","Sepal.W.","Petal.L.","Petal.W.")], main="Fisher's Iris flowers", pch=22,
bg=c("red", "yellow", "blue")[unclass(Data$Sp)])
mydata.lda <- lda(Sp ~ ., Data, prior = c(1,1,1)/3, subset = learn)
mydata.lda <- lda(Sp ~ ., prior = c(1,1,1)/3, data = mydata.learn)
mydata.lda
plot(mydata.lda)
d1LDA <- mydata.learn[,1] * mydata.lda$scaling[1] + mydata.learn[,2] * mydata.lda$scaling[2] +
mydata.learn[,3] * mydata.lda$scaling[3] + mydata.learn[,4] * mydata.lda$scaling[4]
predict(mydata.lda, mydata.learn)$class
predict(mydata.lda, mydata.learn)$posterior # the posterior probabilities
mydata.lda.cv <- lda(Sp ~ ., prior = c(1,1,1)/3, data = Data, subset=learn, CV=TRUE)
summary(mydata.lda.cv$class)
(tab <- table(Data$Sp[learn], mydata.lda.cv$class))
(error.LOOCV <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
mydata.lda.cv.noPetal.W <- update(mydata.lda.cv, . ~ . - Petal.W.)
(tab <- table(Data$Sp[learn], mydata.lda.cv.noPetal.W$class))
(error.LOOCV <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
mydata.qda.cv <- qda(Sp ~ ., prior = c(1,1,1)/3, data = Data, subset=learn, CV=TRUE)
summary(mydata.qda.cv$class)
(tab <- table(Data$Sp[learn], mydata.lda.cv$class))
(error.LOOCV <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
mydata.lda <- lda(Sp ~ ., Data, prior = c(1,1,1)/3, subset = learn)
lda.predictions <- predict(mydata.lda, mydata.test)
lda.predictions$class
(tab <- table(Data$Sp[-learn], lda.predictions$class))
(error.TE <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
wine <- read.table("Wine/wine.data", sep=",", dec=".", header=FALSE)
dim(wine)
colnames(wine) <- c('Wine.type','Alcohol','Malic.acid','Ash','Alcalinity.of.ash','Magnesium','Total.phenols',
'Flavanoids','Nonflavanoid.phenols','Proanthocyanins','Color.intensity','Hue','OD280/OD315','Proline')
wine$Wine.type <- as.factor(wine$Wine.type)
summary(wine)
plot(subset(wine,select=-Wine.type),col=unclass(wine$Wine.type))
lda.model <- lda (Wine.type ~ ., data = wine)
lda.model
plot(lda.model)
wine.pred <- predict(lda.model)
plot(wine.pred$x,type="n")
text(wine.pred$x,labels=as.character(rownames(wine.pred$x)),col=as.integer(wine$Wine.type))
legend('bottomright', c("Cultivar 1","Cultivar 2","Cultivar 3"), lty=1, col=c('black', 'red', 'green'), bty='n', cex=.75)
plot.mean <- function (class)
{
m1 <- mean(subset(wine.pred$x[,1],wine$Wine.type==class))
m2 <- mean(subset(wine.pred$x[,2],wine$Wine.type==class))
print(c(m1,m2))
points(m1,m2,pch=16,cex=2,col=as.integer(class))
}
plot.mean ('1')
plot.mean ('2')
plot.mean ('3')
table(wine$Wine.type, wine.pred$class)
wine.predcv <- update(lda.model,CV=TRUE)
head(wine.predcv$posterior)
print(table(wine$Wine.type,wine.predcv$class))
qda.model <- qda (Wine.type ~ ., data = wine)
qda.model
wine.pred <- predict(qda.model)
table(wine$Wine.type, wine.pred$class)
wine.predcv <- update(qda.model,CV=TRUE)
head(wine.predcv$posterior)
print(table(wine$Wine.type,wine.predcv$class))
set.seed(1111)
Admis <- read.csv("Admissions.csv")
head(Admis)
summary(Admis)
Admis$admit <- factor(Admis$admit, labels=c("No","Yes"))
Admis$rank <- factor(Admis$rank)
summary(Admis)
xtabs(~admit + rank, data = Admis)
N <- nrow(Admis)
learn <- sample(1:N, round(2*N/3))
nlearn <- length(learn)
ntest <- N - nlearn
Admis.logreg <- glm (admit ~ gre + gpa + rank, data = Admis[learn,], family = "binomial")
summary(Admis.logreg)
Admis.logreg.step <- step (Admis.logreg)
Admis.logreg <- glm (Admis.logreg.step$formula, data = Admis[learn,], family=binomial)
summary(Admis.logreg)
exp(Admis.logreg$coefficients)
plot (Admis.logreg$linear.predictors, Admis.logreg$fitted.values, col=Admis[,"admit"])
glfpred<-NULL
glfpred[Admis.logreg$fitted.values<0.5]<-0
glfpred[Admis.logreg$fitted.values>=0.5]<-1
(tab <- with(Admis, table(admit[learn],glfpred)))
error.learn <- 100*(1-sum(diag(tab))/nlearn)
error.learn
glft <- predict(Admis.logreg, newdata=Admis[-learn,], type="response")
glfpredt <- NULL
glfpredt[glft<0.5]<-0
glfpredt[glft>=0.5]<-1
(tab <- with(Admis, table(admit[-learn],glfpredt)))
error.test <- 100*(1-sum(diag(tab))/ntest)
error.test
table(Admis$admit)[2]/N
library(kernlab)
data(spam)
spam[,55:57] <- as.matrix(log10(spam[,55:57]+1))
spam2 <- spam[spam$george==0,]
spam2 <- spam2[spam2$num650==0,]
spam2 <- spam2[spam2$hp==0,]
spam2 <- spam2[spam2$hpl==0,]
george.vars <- 25:28
spam2 <- spam2[,-george.vars]
moneys.vars <- c(16,17,20,24)
spam3 <- data.frame( spam2[,-moneys.vars], spam2[,16]+spam2[,17]+spam2[,20]+spam2[,24])
colnames(spam3)[51] <- "about.money"
dim(spam3)
summary(spam3)
set.seed (4321)
N <- nrow(spam3)
learn <- sample(1:N, round(0.67*N))
nlearn <- length(learn)
ntest <- N - nlearn
spamM1 <- glm (type ~ ., data=spam3[learn,], family=binomial)
spamM1.AIC <- step (spamM1)
## We define now a convenience function:
# 'P' is a parameter; whenever our filter assigns spam with probability at least P then we predict spam
spam.accs <- function (P=0.5)
{
## Compute accuracy in learning data
spamM1.AICpred <- NULL
spamM1.AICpred[spamM1.AIC$fitted.values<P] <- 0
spamM1.AICpred[spamM1.AIC$fitted.values>=P] <- 1
spamM1.AICpred <- factor(spamM1.AICpred, labels=c("nonspam","spam"))
cat("TRAINING ERROR:")
print(M1.TRtable <- table(Truth=spam3[learn,]$type,Pred=spamM1.AICpred))
print(100*(1-sum(diag(M1.TRtable))/nlearn))
## Compute accuracy in test data
gl1t <- predict(spamM1.AIC, newdata=spam3[-learn,],type="response")
gl1predt <- NULL
gl1predt[gl1t<P] <- 0
gl1predt[gl1t>=P] <- 1
gl1predt <- factor(gl1predt, labels=c("nonspam","spam"))
cat("TESTING ERROR:")
print(M1.TEtable <- table(Truth=spam3[-learn,]$type,Pred=gl1predt))
print(100*(1-sum(diag(M1.TEtable))/ntest))
}
spam.accs()
spam.accs(0.7)
Spam.LDA.predcv <- lda(type ~ ., data=spam3[learn,],CV=TRUE)
(tab <- table(spam3[learn,]$type, Spam.LDA.predcv$class))
(error.LOOCV <- 100*(1-sum(tab[row(tab)==col(tab)])/sum(tab)))
Spam.QDA.predcv <- qda(type ~ ., data=spam3[learn,],CV=TRUE)
library(klaR)
Spam.RDA.predcv <- rda (type ~ ., data=spam3[learn,], crossval = TRUE)
Spam.RDA.predcv$error.rate
Spam.RDA.predcv$regularization
RDA.preds <- predict (Spam.RDA.predcv, spam3[-learn,])
addmargins(tab <- table(RDA.preds$class, spam3[-learn,]$type,  dnn=c("Pred", "True")))
sum(tab[row(tab)==col(tab)])/sum(tab)
set.seed (6046)
deposit <- read.table ("Bank Marketing/bank-full.csv", header=TRUE, stringsAsFactors=TRUE, sep=";")
dim(deposit)
summary(deposit)
table(deposit$job)
table(deposit$education)
table(deposit$month)
hist(deposit$duration)
hist(log(deposit$duration))
deposit$duration <- log(deposit$duration+0.001)
colnames(deposit)[ncol(deposit)] <- "subscribed"
dim(deposit)
summary(deposit)
library(reshape2)
library(ggplot2)
d.cont <- melt(deposit[,c("age","balance","duration","campaign","pdays","previous")])
ggplot(d.cont,aes(x = value)) +
facet_wrap(~variable,scales = "free") + geom_histogram()
d.categ <- melt(deposit, measure.vars=c("job","marital","education","housing","loan","contact","default", "poutcome"))
ggplot(d.categ,aes(x = value)) + facet_wrap(~variable,scales = "free") + geom_bar()
N <- nrow(deposit)
all.indexes <- 1:N
learn.indexes <- sample(1:N, round(2*N/3))
test.indexes <- all.indexes[-learn.indexes]
learn.data <- deposit[learn.indexes,]
nlearn <- length(learn.indexes)
ntest <- N - nlearn
...
set.seed (6046)
deposit <- read.table ("Bank Marketing/bank-full.csv", header=TRUE, stringsAsFactors=TRUE, sep=";")
dim(deposit)
summary(deposit)
table(deposit$job)
table(deposit$education)
table(deposit$month)
hist(deposit$duration)
hist(log(deposit$duration))
deposit$duration <- log(deposit$duration+0.001)
colnames(deposit)[ncol(deposit)] <- "subscribed"
dim(deposit)
summary(deposit)
library(reshape2)
library(ggplot2)
d.cont <- melt(deposit[,c("age","balance","duration","campaign","pdays","previous")])
ggplot(d.cont,aes(x = value)) +
facet_wrap(~variable,scales = "free") + geom_histogram()
d.cont
N <- nrow(deposit)
all.indexes <- 1:N
learn.indexes <- sample(1:N, round(2*N/3))
test.indexes <- all.indexes[-learn.indexes]
learn.data <- deposit[learn.indexes,]
nlearn <- length(learn.indexes)
ntest <- N - nlearn
